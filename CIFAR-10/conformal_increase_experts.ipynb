{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Increase Experts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9216, 11])\n",
      "torch.Size([9216, 12])\n",
      "torch.Size([9216, 14])\n",
      "torch.Size([9216, 16])\n",
      "torch.Size([9216, 18])\n"
     ]
    }
   ],
   "source": [
    "# === OvA ===\n",
    "n_classes = 10\n",
    "confs = []\n",
    "exps = []\n",
    "true = []\n",
    "path = \"ova_increase_experts/\"\n",
    "n_experts = [1, 2, 4, 6, 8]\n",
    "for n in n_experts:\n",
    "    model_name = '_' + str(n) + '_experts'\n",
    "    with open(path + 'confidence_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        conf = json.loads(json.load(f))\n",
    "    with open(path + 'expert_predictions_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        exp_pred = json.loads(json.load(f))\n",
    "    with open(path + 'true_label_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        true_label = json.loads(json.load(f))\n",
    "    true.append(true_label['test'])\n",
    "    exps.append(exp_pred['test'])\n",
    "    c = torch.tensor(conf['test'])\n",
    "    print(c.shape)\n",
    "    # DANI Correction ===\n",
    "    c = c.sigmoid()\n",
    "    # DANI Correction ===\n",
    "    confs.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Increase confidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# === OvA ===\n",
    "n_classes = 10\n",
    "random_expert_idx = 0\n",
    "confs = []\n",
    "exps = []\n",
    "true = []\n",
    "path = \"ova_increase_confidence/\"\n",
    "n_experts = 4\n",
    "p_experts = [0.2, 0.4, 0.6, 0.8, 0.95]\n",
    "for p in p_experts:\n",
    "    model_name = '_' + str(p) + '_confidence'\n",
    "    with open(path + 'confidence_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        conf = json.loads(json.load(f))\n",
    "    with open(path + 'expert_predictions_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        exp_pred = json.loads(json.load(f))\n",
    "    with open(path + 'true_label_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        true_label = json.loads(json.load(f))\n",
    "    true.append(true_label['test'])\n",
    "    exps.append(exp_pred['test'])\n",
    "    c = torch.tensor(conf['test'])\n",
    "    # DANI Correction ===\n",
    "    c = c.sigmoid()\n",
    "    # DANI Correction ===\n",
    "    confs.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1 expert rando, 3 with prob 0.8 correct\n",
    "probs = confs[-2]\n",
    "experts = exps[-2]\n",
    "experts = experts[::-1]  # reverse order!\n",
    "y_true = true[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N val:7372\n",
      "N test:1844\n"
     ]
    }
   ],
   "source": [
    "n_val = int(0.8 * len(y_true))\n",
    "n_test = len(y_true) - n_val\n",
    "print(\"N val:{}\".format(n_val))\n",
    "print(\"N test:{}\".format(n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4832)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. get Q_hat\n",
    "\n",
    "n_classes_exp = n_classes + n_experts\n",
    "probs_val = probs[:n_val, 10:]\n",
    "\n",
    "# experts_val = experts[::-1]  # IMPORTANT! swap to match prob ordering\n",
    "experts_val = experts\n",
    "experts_val = [exp[:n_val] for exp in experts_val]\n",
    "\n",
    "y_true_val = y_true[:n_val]\n",
    "\n",
    "# === Only on deferred samples\n",
    "_, predicted = torch.max(probs[:n_val].data, 1)\n",
    "r = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "# Filter \n",
    "probs_val = probs_val[r]\n",
    "experts_val = [np.array(exp)[r] for exp in experts_val]\n",
    "y_true_val = np.array(y_true_val)[r]\n",
    "\n",
    "# Model expert probs ===\n",
    "# Sort J model outputs for experts\n",
    "probs_experts = probs[:n_val, 10:]\n",
    "probs_experts = probs_experts[r]\n",
    "sort, pi = probs_experts.sort(dim=1, descending=True)\n",
    "\n",
    "# Correctness experts ===\n",
    "# Check if experts are correct \n",
    "correct_exp = (np.array(experts_val) == np.array(y_true_val)).T\n",
    "# idx for correct experts: [[0,1,2], [1,2], [], ...]\n",
    "correct_exp_idx = [np.where(correct_exp_i)[0] for correct_exp_i in correct_exp]\n",
    "\n",
    "# obtain the last expert to be retrieved. If empty, then add all values.\n",
    "# indexes are not the real expert index, but the sorted indexes, e.g. [[1, 0 ,2],  [1,0], [], ...]\n",
    "pi_corr_exp = [probs_experts[i, corr_exp].sort(descending=True)[1] for i, corr_exp in enumerate(correct_exp)]\n",
    "pi_corr_exp_stop = [pi_corr_exp_i[-1] if len(pi_corr_exp_i)!=0 else -1 for pi_corr_exp_i in pi_corr_exp]  # last expert\n",
    "\n",
    "# obtain real expert index back, e.g. [2,1,-1,...]\n",
    "pi_stop = [correct_exp_idx[i][pi_corr_exp_stop_i] if len(correct_exp_idx[i])!=0 else -1 for i, pi_corr_exp_stop_i in enumerate(pi_corr_exp_stop)]\n",
    "\n",
    "\n",
    "# =========\n",
    "n_val = n_val\n",
    "alpha = 0.1\n",
    "scores = sort.cumsum(dim=1).gather(1, pi.argsort(1))[range(len(torch.tensor(pi_stop))), torch.tensor(pi_stop)]\n",
    "qhat = torch.quantile(scores, np.ceil((r.sum() + 1) * (1 - alpha)) / r.sum(), interpolation=\"higher\")\n",
    "\n",
    "qhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 4\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "probs_test = probs[n_val:, n_classes:]\n",
    "experts_test = [exp[n_val:] for exp in experts]\n",
    "y_true_test = y_true[n_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 0, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 0]),\n",
       " array([1, 2, 0]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 0, 1, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([1, 0]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 1]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([0, 2, 1, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0]),\n",
       " array([1, 0, 2]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0]),\n",
       " array([1, 0, 2]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0]),\n",
       " array([2, 1, 0, 3]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2]),\n",
       " array([1, 2, 0]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([0, 1, 2]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 0]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0]),\n",
       " array([0, 2, 1, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 0, 2]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([1, 2, 0, 3]),\n",
       " array([1, 0, 2, 3]),\n",
       " array([0, 1, 2]),\n",
       " array([2, 1, 0, 3])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Only on deferred samples\n",
    "_, predicted = torch.max(probs[n_val:].data, 1)\n",
    "r_test = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "# Filter \n",
    "probs_test = probs_test[r_test]\n",
    "experts_test = [np.array(exp)[r_test] for exp in experts_test]\n",
    "y_true_test = np.array(y_true_test)[r_test]\n",
    "\n",
    "# Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "sort_test, pi_test = probs_test.sort(dim=1, descending=True)\n",
    "# Get last sorted index to be below Q_hat\n",
    "pi_stop = (sort_test.cumsum(dim=1) <= qhat).sum(axis=1)\n",
    "\n",
    "# Prediction sets\n",
    "prediction_sets = [(pi_test[i][:(pi_stop[i])]).numpy() for i in range(pi_stop.shape[0])]  # not allow empty sets\n",
    "prediction_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 0, 1, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 0, 1, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 2, 1, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [2, 1, 0, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 2, 1, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [2, 1, 0, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy w/o Conformal on deferred samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_sys = 0\n",
    "exp = 0\n",
    "exp_total = 0\n",
    "total = 0\n",
    "real_total = 0\n",
    "alone_correct = 0\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "expert_total_dic = {k: 0 for k in range(len(experts_test))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 expert rando, 3 with prob 0.9 correct\n",
    "probs = confs[-2]\n",
    "experts = exps[-2]\n",
    "experts = experts[::-1] \n",
    "y_true = true[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 4\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "probs_test = probs[n_val:]\n",
    "experts_test = [exp[n_val:] for exp in experts]\n",
    "y_true_test = y_true[n_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_true_test\n",
    "\n",
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)\n",
    "for i in range(0, n_test):\n",
    "    r = (predicted[i].item() >= n_classes_exp - len(experts_test))\n",
    "    alone_correct += (prediction[i] == labels[i]).item()\n",
    "    if r == 0:\n",
    "        total += 1\n",
    "        correct += (predicted[i] == labels[i]).item()\n",
    "        correct_sys += (predicted[i] == labels[i]).item()\n",
    "        \n",
    "    if r == 1:\n",
    "        # Conformal prediction ===        \n",
    "        # Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "        sort_i, pi_i = probs_test[0,n_classes:].sort(descending=True)\n",
    "        # Get last sorted index to be below Q_hat\n",
    "        pi_stop_i = (sort_i.cumsum(dim=0) <= qhat).sum()\n",
    "\n",
    "        # Prediction sets\n",
    "        prediction_set_i = (pi_i[:(pi_stop_i)]).numpy()  # not allow empty sets        \n",
    "        \n",
    "        ensemble_expert_pred_i = np.array(experts_test)[prediction_set_i][:, i]\n",
    "        exp_prediction = stats.mode(ensemble_expert_pred_i).mode\n",
    "        # Conformal prediction ===\n",
    "        \n",
    "        # Deferral accuracy: No matter expert ===\n",
    "        exp += (exp_prediction == labels[i])\n",
    "        exp_total += 1\n",
    "        # Individual Expert Accuracy ===\n",
    "        # expert_correct_dic[deferred_exp] += (exp_prediction == labels[i].item())\n",
    "        # expert_total_dic[deferred_exp] += 1\n",
    "        #\n",
    "        correct_sys += (exp_prediction == labels[i])\n",
    "    real_total += 1\n",
    "    \n",
    "#  ===  Coverage  === #    \n",
    "cov = str(total) + str(\" out of\") + str(real_total)\n",
    "\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "             in range(len(experts_test))}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([83.07683787]),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * exp / (exp_total + 0.0002),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90.34707158])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * correct_sys / real_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/o Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_sys = 0\n",
    "exp = 0\n",
    "exp_total = 0\n",
    "total = 0\n",
    "real_total = 0\n",
    "alone_correct = 0\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "expert_total_dic = {k: 0 for k in range(len(experts_test))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 expert rando, 3 with prob 0.9 correct\n",
    "probs = confs[-2]\n",
    "experts = exps[-2]\n",
    "experts = experts[::-1] \n",
    "y_true = true[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 4\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "probs_test = probs[n_val:]\n",
    "experts_test = [exp[n_val:] for exp in experts]\n",
    "y_true_test = y_true[n_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_true_test\n",
    "\n",
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)\n",
    "for i in range(0, n_test):\n",
    "    r = (predicted[i].item() >= n_classes_exp - len(experts_test))\n",
    "    alone_correct += (prediction[i] == labels[i]).item()\n",
    "    if r == 0:\n",
    "        total += 1\n",
    "        correct += (predicted[i] == labels[i]).item()\n",
    "        correct_sys += (predicted[i] == labels[i]).item()\n",
    "        \n",
    "    if r == 1:\n",
    "        deferred_exp = (predicted[i] - n_classes).item()  # reverse order, as in loss function\n",
    "        # deferred_exp = ((n_classes - 1) - predicted[i]).item()  # reverse order, as in loss function\n",
    "        exp_prediction = experts_test[deferred_exp][i]\n",
    "        #\n",
    "        # Deferral accuracy: No matter expert ===\n",
    "        exp += (exp_prediction == labels[i])\n",
    "        exp_total += 1\n",
    "        # Individual Expert Accuracy ===\n",
    "        expert_correct_dic[deferred_exp] += (exp_prediction == labels[i])\n",
    "        expert_total_dic[deferred_exp] += 1\n",
    "        #\n",
    "        correct_sys += (exp_prediction == labels[i])\n",
    "    real_total += 1\n",
    "    \n",
    "#  ===  Coverage  === #    \n",
    "cov = str(total) + str(\" out of\") + str(real_total)\n",
    "\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "             in range(len(experts_test))}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expert_0': 76.18975057380406,\n",
       " 'expert_1': 72.95588307436091,\n",
       " 'expert_2': 79.99893334755537,\n",
       " 'expert_3': 0.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73.84607810658656,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * exp / (exp_total + 0.0002),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.3709327548807"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * correct_sys / real_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy w Conformal on deferred samples"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 27,
=======
   "execution_count": 129,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9849)"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 27,
=======
     "execution_count": 129,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qhat"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
=======
   "execution_count": 130,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "tensor([[0.8161, 0.8437, 0.7678, 0.1060],\n",
       "        [0.8036, 0.7980, 0.8149, 0.1306],\n",
       "        [0.8327, 0.8606, 0.7507, 0.0810],\n",
       "        ...,\n",
       "        [0.8224, 0.8567, 0.8424, 0.0901],\n",
       "        [0.2448, 0.2486, 0.2371, 0.0964],\n",
       "        [0.7957, 0.8283, 0.7771, 0.1163]])"
      ]
     },
     "execution_count": 28,
=======
       "tensor([[0.5999, 0.6590, 0.6888, 0.0981],\n",
       "        [0.6246, 0.6823, 0.6741, 0.0745],\n",
       "        [0.6406, 0.6843, 0.7085, 0.1032],\n",
       "        ...,\n",
       "        [0.5885, 0.6231, 0.6423, 0.1091],\n",
       "        [0.2391, 0.2695, 0.2638, 0.1007],\n",
       "        [0.6083, 0.6606, 0.7000, 0.0941]])"
      ]
     },
     "execution_count": 130,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_test[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
