{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1079aba8-2e21-41d4-85c6-aaf7fd030cc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conformal Experiment: Increase Confidence Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f62873-3ac6-4961-934b-3ac2722fc73d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === Softmax ===\n",
    "n_classes = 10\n",
    "probs_softmax = []\n",
    "confs = []\n",
    "exps = []\n",
    "true = []\n",
    "\n",
    "path = \"softmax_gradual_overlap/\"\n",
    "n_experts = 10\n",
    "p_out = [0.1,0.2,0.4,0.6,0.8,0.95,1.0]\n",
    "for p in p_out:\n",
    "    model_name = '_p_out_' + str(p)\n",
    "    with open(path + 'confidence_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        conf = json.loads(json.load(f))\n",
    "    with open(path + 'expert_predictions_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        exp_pred = json.loads(json.load(f))\n",
    "    with open(path + 'true_label_multiple_experts' + model_name + '.txt', 'r') as f:\n",
    "        true_label = json.loads(json.load(f))\n",
    "    true.append(true_label['test'])\n",
    "    exps.append(exp_pred['test'])\n",
    "    c = torch.tensor(conf['test'])\n",
    "    print(c.shape)\n",
    "    # DANI Correction ===\n",
    "    c = c.softmax(dim=1)\n",
    "    probs_softmax.append(c)\n",
    "    # DANI Correction ===\n",
    "\n",
    "    temp = 0\n",
    "    for i in range(n_experts):\n",
    "        temp += c[:, (n_classes + n_experts) - (i + 1)]\n",
    "    prob = c / (1.0 - temp).unsqueeze(-1)\n",
    "    confs.append(prob)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use c, not conf, because we do not apply temperature scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1 expert rando, 3 with prob 0.8 correct\n",
    "probs = probs_softmax[-1]\n",
    "experts = exps[-1]\n",
    "# experts = experts[::-1]  # reverse order!\n",
    "y_true = true[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs_softmax[-1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.array(experts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_true[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_val = int(0.8 * len(y_true))\n",
    "n_test = len(y_true) - n_val\n",
    "print(\"N val:{}\".format(n_val))\n",
    "print(\"N test:{}\".format(n_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 2. get Q_hat\n",
    "\n",
    "n_classes_exp = n_classes + n_experts\n",
    "probs_val = probs[:n_val, 10:]\n",
    "\n",
    "# experts_val = experts[::-1]  # IMPORTANT! swap to match prob ordering\n",
    "experts_val = experts\n",
    "experts_val = [exp[:n_val] for exp in experts_val]\n",
    "\n",
    "y_true_val = y_true[:n_val]\n",
    "\n",
    "# === Only on deferred samples\n",
    "_, predicted = torch.max(probs[:n_val].data, 1)\n",
    "r = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "# Filter \n",
    "probs_val = probs_val[r]\n",
    "experts_val = [np.array(exp)[r] for exp in experts_val]\n",
    "y_true_val = np.array(y_true_val)[r]\n",
    "\n",
    "# Model expert probs ===\n",
    "# Sort J model outputs for experts\n",
    "probs_experts = probs[:n_val, 10:]\n",
    "probs_experts = probs_experts[r]\n",
    "sort, pi = probs_experts.sort(dim=1, descending=True)\n",
    "\n",
    "# Correctness experts ===\n",
    "# Check if experts are correct \n",
    "correct_exp = (np.array(experts_val) == np.array(y_true_val)).T\n",
    "# idx for correct experts: [[0,1,2], [1,2], [], ...]\n",
    "correct_exp_idx = [np.where(correct_exp_i)[0] for correct_exp_i in correct_exp]\n",
    "\n",
    "# obtain the last expert to be retrieved. If empty, then add all values.\n",
    "# indexes are not the real expert index, but the sorted indexes, e.g. [[1, 0 ,2],  [1,0], [], ...]\n",
    "pi_corr_exp = [probs_experts[i, corr_exp].sort(descending=True)[1] for i, corr_exp in enumerate(correct_exp)]\n",
    "pi_corr_exp_stop = [pi_corr_exp_i[-1] if len(pi_corr_exp_i)!=0 else -1 for pi_corr_exp_i in pi_corr_exp]  # last expert\n",
    "\n",
    "# obtain real expert index back, e.g. [2,1,-1,...]\n",
    "pi_stop = [correct_exp_idx[i][pi_corr_exp_stop_i] if len(correct_exp_idx[i])!=0 else -1 for i, pi_corr_exp_stop_i in enumerate(pi_corr_exp_stop)]\n",
    "\n",
    "\n",
    "# =========\n",
    "n_val = n_val\n",
    "alpha = 0.1\n",
    "scores = sort.cumsum(dim=1).gather(1, pi.argsort(1))[range(len(torch.tensor(pi_stop))), torch.tensor(pi_stop)]\n",
    "qhat = torch.quantile(scores, np.ceil((r.sum() + 1) * (1 - alpha)) / r.sum(), interpolation=\"higher\")\n",
    "\n",
    "qhat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs[:,10:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sort.cumsum(dim=1).gather(1, pi.argsort(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_true_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.array(experts_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 4\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "probs_test = probs[n_val:, n_classes:]\n",
    "experts_test = [exp[n_val:] for exp in experts]\n",
    "y_true_test = y_true[n_val:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs_test.sum(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === Only on deferred samples\n",
    "_, predicted = torch.max(probs[n_val:].data, 1)\n",
    "r_test = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "# Filter \n",
    "probs_test = probs_test[r_test]\n",
    "experts_test = [np.array(exp)[r_test] for exp in experts_test]\n",
    "y_true_test = np.array(y_true_test)[r_test]\n",
    "\n",
    "# Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "sort_test, pi_test = probs_test.sort(dim=1, descending=True)\n",
    "# Get last sorted index to be below Q_hat\n",
    "pi_stop = (sort_test.cumsum(dim=1) <= qhat).sum(axis=1)\n",
    "\n",
    "# Prediction sets\n",
    "prediction_sets = [(pi_test[i][:(pi_stop[i])]).numpy() for i in range(pi_stop.shape[0])]  # not allow empty sets\n",
    "prediction_sets[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = np.array([len(prediction_sets_i) for prediction_sets_i in prediction_sets])\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.array(experts_test)[:,:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy w/o Conformal on deferred samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_sys = 0\n",
    "exp = 0\n",
    "exp_total = 0\n",
    "total = 0\n",
    "real_total = 0\n",
    "alone_correct = 0\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "expert_total_dic = {k: 0 for k in range(len(experts_test))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1 expert rando, 3 with prob 0.9 correct\n",
    "probs = probs_softmax[-1]\n",
    "experts = exps[-1]\n",
    "# experts = experts[::-1] \n",
    "y_true = true[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 4\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "probs_test = probs[n_val:]\n",
    "experts_test = [exp[n_val:] for exp in experts]\n",
    "y_true_test = y_true[n_val:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### w Conformal Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs.sum(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = y_true_test\n",
    "\n",
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)\n",
    "for i in range(0, n_test):\n",
    "    r = (predicted[i].item() >= n_classes_exp - len(experts_test))\n",
    "    alone_correct += (prediction[i] == labels[i]).item()\n",
    "    if r == 0:\n",
    "        total += 1\n",
    "        correct += (predicted[i] == labels[i]).item()\n",
    "        correct_sys += (predicted[i] == labels[i]).item()\n",
    "        \n",
    "    if r == 1:\n",
    "        # Conformal prediction ===        \n",
    "        # Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "        sort_i, pi_i = probs_test[i,n_classes:].sort(descending=True)\n",
    "        # Get last sorted index to be below Q_hat\n",
    "        pi_stop_i = (sort_i.cumsum(dim=0) <= qhat).sum()\n",
    "\n",
    "        # Prediction sets\n",
    "        prediction_set_i = (pi_i[:(pi_stop_i)]).numpy()  # not allow empty sets        \n",
    "#         print(len(prediction_set_i))\n",
    "        ensemble_expert_pred_i = np.array(experts_test)[prediction_set_i][:, i]\n",
    "        exp_prediction = stats.mode(ensemble_expert_pred_i).mode\n",
    "        # Conformal prediction ===\n",
    "        \n",
    "        # Deferral accuracy: No matter expert ===\n",
    "        exp += (exp_prediction == labels[i])\n",
    "        exp_total += 1\n",
    "        # Individual Expert Accuracy ===\n",
    "        # expert_correct_dic[deferred_exp] += (exp_prediction == labels[i].item())\n",
    "        # expert_total_dic[deferred_exp] += 1\n",
    "        #\n",
    "        correct_sys += (exp_prediction == labels[i])\n",
    "    real_total += 1\n",
    "    \n",
    "#  ===  Coverage  === #    \n",
    "cov = str(total) + str(\" out of\") + str(real_total)\n",
    "\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "             in range(len(experts_test))}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sort_i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Cov: {}\".format(100 * (total/real_total)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expert_accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "100 * exp / (exp_total + 0.0002),"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "100 * correct_sys / real_total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### w/o Conformal Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_sys = 0\n",
    "exp = 0\n",
    "exp_total = 0\n",
    "total = 0\n",
    "real_total = 0\n",
    "alone_correct = 0\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "expert_total_dic = {k: 0 for k in range(len(experts_test))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1 expert rando, 3 with prob 0.9 correct\n",
    "probs = probs_softmax[-1]\n",
    "experts = exps[-1]\n",
    "# experts = experts[::-1] \n",
    "y_true = true[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 4\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "probs_test = probs[n_val:]\n",
    "experts_test = [exp[n_val:] for exp in experts]\n",
    "y_true_test = y_true[n_val:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = y_true_test\n",
    "\n",
    "# Predicted value \n",
    "_, predicted = torch.max(probs_test.data, 1)\n",
    "# Classifier alone prediction\n",
    "_, prediction = torch.max(probs_test.data[:, :(n_classes_exp - n_experts)],1)\n",
    "for i in range(0, n_test):\n",
    "    r = (predicted[i].item() >= n_classes_exp - len(experts_test))\n",
    "    alone_correct += (prediction[i] == labels[i]).item()\n",
    "    if r == 0:\n",
    "        total += 1\n",
    "        correct += (predicted[i] == labels[i]).item()\n",
    "        correct_sys += (predicted[i] == labels[i]).item()\n",
    "        \n",
    "    if r == 1:\n",
    "        deferred_exp = (predicted[i] - n_classes).item()  # reverse order, as in loss function\n",
    "        # print(\"def exp: {}\".format(deferred_exp))\n",
    "        # deferred_exp = ((n_classes - 1) - predicted[i]).item()  # reverse order, as in loss function\n",
    "        exp_prediction = experts_test[deferred_exp][i]\n",
    "        # print(\"exp pred: {}\".format(exp_prediction))\n",
    "        #\n",
    "        # Deferral accuracy: No matter expert ===\n",
    "        exp += (exp_prediction == labels[i])\n",
    "        # print(\"label: {}\".format(labels[i]))\n",
    "\n",
    "        exp_total += 1\n",
    "        # Individual Expert Accuracy ===\n",
    "        expert_correct_dic[deferred_exp] += (exp_prediction == labels[i])\n",
    "        expert_total_dic[deferred_exp] += 1\n",
    "        #\n",
    "        correct_sys += (exp_prediction == labels[i])\n",
    "    real_total += 1\n",
    "    \n",
    "#  ===  Coverage  === #    \n",
    "cov = str(total) + str(\" out of\") + str(real_total)\n",
    "\n",
    "#  === Individual Expert Accuracies === #\n",
    "expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "             in range(len(experts_test))}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(predicted[i] - n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expert_accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "100 * exp / (exp_total + 0.0002),"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "100 * correct_sys / real_total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy w Conformal on deferred samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qhat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs_test[:,10:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Metric Calculation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_expert_prediction(experts, prediction_set_i, method=\"voting\"):\n",
    "    ensemble_expert_pred_i = np.array(experts_test)[prediction_set_i][:, i]\n",
    "    if method == \"voting\":\n",
    "        exp_prediction = stats.mode(ensemble_expert_pred_i).mode if len(ensemble_expert_pred_i)!=0 else []\n",
    "        \n",
    "    if method == \"last\": \n",
    "        exp_prediction = ensemble_expert_pred_i[-1] if len(ensemble_expert_pred_i)!=0 else []\n",
    "        \n",
    "    if method == \"random\":\n",
    "        idx = np.random.randint(len(ensemble_expert_pred_i)) if len(ensemble_expert_pred_i)!=0 else -1\n",
    "        exp_prediction = ensemble_expert_pred_i[idx] if idx!=-1 else []\n",
    "        \n",
    "    return exp_prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Method dict ===\n",
    "method_list = [\"last\", \"random\", \"voting\"]\n",
    "method_dict = {\"last\": [],\n",
    "               \"random\": [],\n",
    "               \"voting\": []}\n",
    "\n",
    "\n",
    "p_out = [0.1,0.2,0.4,0.6,0.8,0.95,1.0]\n",
    "alpha = 0.1\n",
    "n_classes = 10\n",
    "n_experts = 10\n",
    "n_classes_exp = n_classes + n_experts\n",
    "\n",
    "for method in method_list:\n",
    "    \n",
    "    print(\"Method: {}\\n\".format(method))\n",
    "    for i, p_exp in enumerate(p_out):\n",
    "        # =============\n",
    "        # = Get Probs =\n",
    "        # =============\n",
    "\n",
    "        probs = confs[i]\n",
    "        experts = exps[i]\n",
    "        experts = experts[::-1]  # reverse order!\n",
    "        y_true = true[-i]\n",
    "\n",
    "        # Val/Calibration ===\n",
    "        probs_val = probs[:n_val, n_classes:]\n",
    "        experts_val = [exp[:n_val] for exp in experts]\n",
    "        y_true_val = y_true[:n_val]\n",
    "\n",
    "        # Test ===\n",
    "        probs_test = probs[n_val:, n_classes:]\n",
    "        experts_test = [exp[n_val:] for exp in experts]\n",
    "        y_true_test = y_true[n_val:]\n",
    "\n",
    "\n",
    "        # =============\n",
    "        # = Conformal =\n",
    "        # =============\n",
    "\n",
    "        # Calculate Q_hat ===\n",
    "\n",
    "        # === Only on deferred samples\n",
    "        _, predicted = torch.max(probs[:n_val].data, 1)\n",
    "        r = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "        # Filter \n",
    "        probs_experts = probs_val[r]\n",
    "        experts_val = [np.array(exp)[r] for exp in experts_val]\n",
    "        y_true_val = np.array(y_true_val)[r]\n",
    "\n",
    "        # Model expert probs ===\n",
    "        # Sort J model outputs for experts\n",
    "        sort, pi = probs_experts.sort(dim=1, descending=True)\n",
    "\n",
    "        # Correctness experts ===\n",
    "        # Check if experts are correct \n",
    "        correct_exp = (np.array(experts_val) == np.array(y_true_val)).T\n",
    "        # idx for correct experts: [[0,1,2], [1,2], [], ...]\n",
    "        correct_exp_idx = [np.where(correct_exp_i)[0] for correct_exp_i in correct_exp]\n",
    "\n",
    "        # obtain the last expert to be retrieved. If empty, then add all values.\n",
    "        # indexes are not the real expert index, but the sorted indexes, e.g. [[1, 0 ,2],  [1,0], [], ...]\n",
    "        pi_corr_exp = [probs_experts[i, corr_exp].sort(descending=True)[1] for i, corr_exp in enumerate(correct_exp)]\n",
    "        pi_corr_exp_stop = [pi_corr_exp_i[-1] if len(pi_corr_exp_i)!=0 else -1 for pi_corr_exp_i in pi_corr_exp]  # last expert\n",
    "\n",
    "        # obtain real expert index back, e.g. [2,1,-1,...]\n",
    "        pi_stop = [correct_exp_idx[i][pi_corr_exp_stop_i] if len(correct_exp_idx[i])!=0 else -1 for i, pi_corr_exp_stop_i in enumerate(pi_corr_exp_stop)]\n",
    "\n",
    "        scores = sort.cumsum(dim=1).gather(1, pi.argsort(1))[range(len(torch.tensor(pi_stop))), torch.tensor(pi_stop)]\n",
    "        n_quantile = r.sum()\n",
    "        qhat = torch.quantile(scores, np.ceil((n_quantile + 1) * (1 - alpha)) / n_quantile, interpolation=\"higher\")\n",
    "\n",
    "        print(\"Q_hat {}: {}\".format(p_exp, qhat))\n",
    "\n",
    "\n",
    "        # =============\n",
    "        # = Metrics =\n",
    "        # =============\n",
    "\n",
    "        # === Initalize ====\n",
    "\n",
    "        correct = 0\n",
    "        correct_sys = 0\n",
    "        exp = 0\n",
    "        exp_total = 0\n",
    "        total = 0\n",
    "        real_total = 0\n",
    "        alone_correct = 0\n",
    "\n",
    "        # Individual Expert Accuracies === #\n",
    "        expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "        expert_total_dic = {k: 0 for k in range(len(experts_test))}\n",
    "\n",
    "        probs_test_exp = probs_test\n",
    "        probs_test_model = probs[n_val:]\n",
    "\n",
    "        # Predicted value \n",
    "        _, predicted = torch.max(probs_test_model.data, 1)\n",
    "\n",
    "        # Classifier alone prediction\n",
    "        _, prediction = torch.max(probs_test_model.data[:, :(n_classes_exp - n_experts)],1)\n",
    "\n",
    "        labels = y_true_test\n",
    "        for i in range(0, n_test):\n",
    "            r = (predicted[i].item() >= n_classes_exp - n_experts)\n",
    "            alone_correct += (prediction[i] == labels[i]).item()\n",
    "\n",
    "            # Non-deferred \n",
    "            if r == 0:\n",
    "                total += 1\n",
    "                correct += (predicted[i] == labels[i]).item()\n",
    "                correct_sys += (predicted[i] == labels[i]).item()\n",
    "\n",
    "            # Deferred \n",
    "            if r == 1:\n",
    "                # Conformal prediction ===        \n",
    "                # Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "                sort_i, pi_i = probs_test_exp[i].sort(descending=True)\n",
    "                # Get last sorted index to be below Q_hat\n",
    "                pi_stop_i = (sort_i.cumsum(dim=0) <= qhat).sum()\n",
    "\n",
    "                # Prediction sets\n",
    "                prediction_set_i = (pi_i[:(pi_stop_i)]).numpy()  # not allow empty sets        \n",
    "\n",
    "\n",
    "\n",
    "                # - Get expert prediction depending on method\n",
    "                # ======\n",
    "                exp_prediction = get_expert_prediction(experts_test, prediction_set_i, method=method)\n",
    "                # ======\n",
    "\n",
    "\n",
    "\n",
    "                # Deferral accuracy: No matter expert ===\n",
    "                exp += (exp_prediction == labels[i])\n",
    "                exp_total += 1\n",
    "                # Individual Expert Accuracy ===\n",
    "                # expert_correct_dic[deferred_exp] += (exp_prediction == labels[i].item())\n",
    "                # expert_total_dic[deferred_exp] += 1\n",
    "                #\n",
    "                correct_sys += (exp_prediction == labels[i])\n",
    "\n",
    "            real_total += 1\n",
    "\n",
    "        #  ===  Coverage  === #    \n",
    "        cov = 100 * total / real_total\n",
    "\n",
    "        #  === Individual Expert Accuracies === #\n",
    "        expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "                     in range(len(experts_test))}\n",
    "\n",
    "        # Add expert accuracies dict\n",
    "        to_print = {\"coverage\": cov,\n",
    "                    \"system_accuracy\": 100 * correct_sys / real_total,\n",
    "                    \"expert_accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                    \"classifier_accuracy\": 100 * correct / (total + 0.0001),\n",
    "                    \"alone_classifier\": 100 * alone_correct / real_total}\n",
    "        print(to_print, flush=True)\n",
    "\n",
    "        # Save to method dict === \n",
    "        method_dict[method].append(to_print)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "# # === Latex Options === #\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "\n",
    "# === Matplotlib Options === #\n",
    "cm = plt.cm.get_cmap('tab10')\n",
    "plot_args = {\"linestyle\": \"-\",\n",
    "             \"marker\": \"o\",\n",
    "             \"markeredgecolor\": \"k\",\n",
    "             \"markersize\": 10,\n",
    "             \"linewidth\": 8\n",
    "             }\n",
    "sns.set_context(\"talk\", font_scale=1.3)\n",
    "fig_size = (6, 6)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_out = [0.1,0.2,0.4,0.6,0.8,0.95,1.0]\n",
    "\n",
    "sys_acc_last = np.array([method_d[\"system_accuracy\"] for method_d in method_dict[\"last\"]])\n",
    "sys_acc_random = np.array([method_d[\"system_accuracy\"] for method_d in method_dict[\"random\"]])\n",
    "sys_acc_voting = np.array([method_d[\"system_accuracy\"] for method_d in method_dict[\"voting\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(p_out, sys_acc_last, label=r\"Last\", **plot_args)\n",
    "ax.plot(p_out, sys_acc_random, label=r\"Random\", **plot_args)\n",
    "ax.plot(p_out, sys_acc_voting, label=r\"Voting\", **plot_args)\n",
    "plt.xticks(p_out, p_out)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'System Acc. ($\\%$)')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"Softmax CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_out = [0.1,0.2,0.4,0.6,0.8,0.95,1.0]\n",
    "\n",
    "exp_acc_last = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict[\"last\"]])\n",
    "exp_acc_random = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict[\"random\"]])\n",
    "exp_acc_voting = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict[\"voting\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(p_out, exp_acc_last, label=r\"Last\", **plot_args)\n",
    "ax.plot(p_out, exp_acc_random, label=r\"Random\", **plot_args)\n",
    "ax.plot(p_out, exp_acc_voting, label=r\"Voting\", **plot_args)\n",
    "plt.xticks(p_out, p_out)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylim([0, 105])\n",
    "plt.ylabel(r'Exp Acc. ($\\%$)')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"Softmax CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_out = [0.1,0.2,0.4,0.6,0.8,0.95,1.0]\n",
    "\n",
    "coverage = np.array([method_d[\"coverage\"] for method_d in method_dict[\"last\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(p_out, coverage, label=r\"Coverage\", **plot_args)\n",
    "plt.xticks(p_out, p_out)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'Model Coverage. ($\\%$)')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"Softmax CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fabcaa-f180-4f47-ab85-14e7f4a10e15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de595611-dc4a-456c-9685-9ea3d2177876",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}