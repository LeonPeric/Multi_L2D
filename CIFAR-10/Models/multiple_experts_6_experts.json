{"batch_size": 1024, "alpha": 1.0, "epochs": 200, "patience": 20, "expert_type": "predict_biasedK", "n_classes": 10, "k": 5, "n_experts": 2, "lr": 0.1, "weight_decay": 0.0005, "warmup_epochs": 20, "loss_type": "softmax", "ckp_dir": "./Models", "experiment_name": "multiple_experts"}