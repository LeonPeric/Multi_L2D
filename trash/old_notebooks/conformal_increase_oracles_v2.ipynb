{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b52bf07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conformal Experiment: Increase Oracles\n",
    "- OvA\n",
    "- Softmax\n",
    "\n",
    "**Experiment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168fe84c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6de73d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_experts = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c81f4f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bb76d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_expert_prediction(experts, prediction_set_i, method=\"voting\"):\n",
    "    ensemble_expert_pred_i = np.array(experts_test)[prediction_set_i][:, i]\n",
    "    if method == \"voting\":\n",
    "        exp_prediction = stats.mode(ensemble_expert_pred_i).mode if len(ensemble_expert_pred_i)!=0 else []\n",
    "        \n",
    "    if method == \"last\": \n",
    "        exp_prediction = ensemble_expert_pred_i[-1] if len(ensemble_expert_pred_i)!=0 else []\n",
    "        \n",
    "    if method == \"random\":\n",
    "        idx = np.random.randint(len(ensemble_expert_pred_i)) if len(ensemble_expert_pred_i)!=0 else -1\n",
    "        exp_prediction = ensemble_expert_pred_i[idx] if idx!=-1 else []\n",
    "        \n",
    "    return exp_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8cdc6-b646-43ad-8415-4662567840b6",
   "metadata": {},
   "source": [
    "## OvA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6321d0-e7f9-4711-93b3-bfddb5445819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc60cb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ova_increase_oracle_v2/confidence_multiple_experts_k_0seed_436.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/75/dsq472pn7sd10570_8hjkj7r0002nk/T/ipykernel_55723/3852035955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_k_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mseed_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'seed_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'confidence_multiple_experts'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseed_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'expert_predictions_multiple_experts'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseed_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ova_increase_oracle_v2/confidence_multiple_experts_k_0seed_436.txt'"
     ]
    }
   ],
   "source": [
    "# === OvA ===\n",
    "confs = []\n",
    "exps = []\n",
    "true = []\n",
    "path = \"ova_increase_oracle_v2/\"\n",
    "\n",
    "k_list = np.arange(0,10)\n",
    "seed = 436\n",
    "for k in k_list:\n",
    "    model_name = '_k_' + str(k)\n",
    "    seed_name = 'seed_' + str(seed)\n",
    "    with open(path + 'confidence_multiple_experts' + model_name + seed_name + '.txt', 'r') as f:\n",
    "        conf = json.loads(json.load(f))\n",
    "    with open(path + 'expert_predictions_multiple_experts' + model_name + seed_name + '.txt', 'r') as f:\n",
    "        exp_pred = json.loads(json.load(f))\n",
    "    with open(path + 'true_label_multiple_experts' + model_name + seed_name + '.txt', 'r') as f:\n",
    "        true_label = json.loads(json.load(f))\n",
    "    true.append(true_label['test'])\n",
    "    exps.append(exp_pred['test'])\n",
    "    c = torch.tensor(conf['test'])\n",
    "    # DANI Correction ===\n",
    "    c = c.sigmoid()\n",
    "    # DANI Correction ===\n",
    "    confs.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f337ec6-1526-4122-8b3e-f9ef3df0f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "(confs[1][1,10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282ad12-8f57-46be-9c8a-fda1fd010c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(confs[1][1,10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52eb62-5b36-48c1-a40b-6382701cf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true[0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa56a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_val = int(0.8 * len(true[0]))\n",
    "n_test = len(true[0]) - n_val\n",
    "print(\"N val:{}\".format(n_val))\n",
    "print(\"N test:{}\".format(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a55f5-2044-416d-95dc-fb88889e2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = np.arange(0,10)\n",
    "alpha = 0.1\n",
    "n_classes = 10\n",
    "n_experts = 10\n",
    "n_classes_exp = n_classes + n_experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0479a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Method dict ===\n",
    "method_list_ova = [\"standard\", \"last\", \"random\", \"voting\"]\n",
    "method_dict_ova = {\"standard\": [],\n",
    "               \"last\": [],\n",
    "               \"random\": [],\n",
    "               \"voting\": []}\n",
    "\n",
    "for method in method_list_ova:\n",
    "    print(\"Method: {}\\n\".format(method))\n",
    "    for i, k in enumerate(k_list):\n",
    "        # =============\n",
    "        # = Get Probs =\n",
    "        # =============\n",
    "        set_size = []\n",
    "\n",
    "        probs = confs[i]\n",
    "        experts = exps[i]\n",
    "        experts = experts[::-1]  # reverse order!\n",
    "        y_true = true[-i]\n",
    "\n",
    "        # Val/Calibration ===\n",
    "        probs_val = probs[:n_val, n_classes:]\n",
    "        experts_val = [exp[:n_val] for exp in experts]\n",
    "        y_true_val = y_true[:n_val]\n",
    "\n",
    "        # Test ===\n",
    "        probs_test = probs[n_val:, n_classes:]\n",
    "        experts_test = [exp[n_val:] for exp in experts]\n",
    "        y_true_test = y_true[n_val:]\n",
    "\n",
    "\n",
    "        # =============\n",
    "        # = Conformal =\n",
    "        # =============\n",
    "\n",
    "        # Calculate Q_hat ===\n",
    "\n",
    "        # === Only on deferred samples\n",
    "        _, predicted = torch.max(probs[:n_val].data, 1)\n",
    "        r = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "        # Filter \n",
    "        probs_experts = probs_val[r]\n",
    "        experts_val = [np.array(exp)[r] for exp in experts_val]\n",
    "        y_true_val = np.array(y_true_val)[r]\n",
    "\n",
    "        # Model expert probs ===\n",
    "        # Sort J model outputs for experts\n",
    "        sort, pi = probs_experts.sort(dim=1, descending=True)\n",
    "\n",
    "        # Correctness experts ===\n",
    "        # Check if experts are correct \n",
    "        correct_exp = (np.array(experts_val) == np.array(y_true_val)).T\n",
    "        # idx for correct experts: [[0,1,2], [1,2], [], ...]\n",
    "        correct_exp_idx = [np.where(correct_exp_i)[0] for correct_exp_i in correct_exp]\n",
    "\n",
    "        # obtain the last expert to be retrieved. If empty, then add all values.\n",
    "        # indexes are not the real expert index, but the sorted indexes, e.g. [[1, 0 ,2],  [1,0], [], ...]\n",
    "        pi_corr_exp = [probs_experts[i, corr_exp].sort(descending=True)[1] for i, corr_exp in enumerate(correct_exp)]\n",
    "        pi_corr_exp_stop = [pi_corr_exp_i[-1] if len(pi_corr_exp_i)!=0 else -1 for pi_corr_exp_i in pi_corr_exp]  # last expert\n",
    "\n",
    "        # obtain real expert index back, e.g. [2,1,-1,...]\n",
    "        pi_stop = [correct_exp_idx[i][pi_corr_exp_stop_i] if len(correct_exp_idx[i])!=0 else -1 for i, pi_corr_exp_stop_i in enumerate(pi_corr_exp_stop)]\n",
    "\n",
    "        scores = sort.cumsum(dim=1).gather(1, pi.argsort(1))[range(len(torch.tensor(pi_stop))), torch.tensor(pi_stop)]\n",
    "        n_quantile = r.sum()\n",
    "        qhat = torch.quantile(scores, np.ceil((n_quantile + 1) * (1 - alpha)) / n_quantile, interpolation=\"higher\")\n",
    "        print(\"Q_hat {}: {}\".format(k, qhat))\n",
    "\n",
    "\n",
    "        # =============\n",
    "        # = Metrics =\n",
    "        # =============\n",
    "\n",
    "        # === Initalize ====\n",
    "\n",
    "        correct = 0\n",
    "        correct_sys = 0\n",
    "        exp = 0\n",
    "        exp_total = 0\n",
    "        total = 0\n",
    "        real_total = 0\n",
    "        alone_correct = 0\n",
    "\n",
    "        # Individual Expert Accuracies === #\n",
    "        expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "        expert_total_dic = {k: 0 for k in range(len(experts_test))}\n",
    "\n",
    "        probs_test_exp = probs_test\n",
    "        probs_test_model = probs[n_val:]\n",
    "\n",
    "        # Predicted value \n",
    "        _, predicted = torch.max(probs_test_model.data, 1)\n",
    "\n",
    "        # Classifier alone prediction\n",
    "        _, prediction = torch.max(probs_test_model.data[:, :(n_classes_exp - n_experts)],1)\n",
    "\n",
    "        labels = y_true_test\n",
    "        for i in range(0, n_test):\n",
    "            r = (predicted[i].item() >= n_classes_exp - n_experts)\n",
    "            alone_correct += (prediction[i] == labels[i]).item()\n",
    "\n",
    "            # Non-deferred \n",
    "            if r == 0:\n",
    "                total += 1\n",
    "                correct += (predicted[i] == labels[i]).item()\n",
    "                correct_sys += (predicted[i] == labels[i]).item()\n",
    "\n",
    "            # Deferred \n",
    "            if r == 1:\n",
    "                # Non Conformal prediction ===\n",
    "                if method==\"standard\":\n",
    "                    deferred_exp = (predicted[i] - n_classes).item()  # reverse order, as in loss function\n",
    "                    exp_prediction = experts_test[deferred_exp][i]\n",
    "                    \n",
    "                    # Conformal prediction ===        \n",
    "                else:                \n",
    "                    # Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "                    sort_i, pi_i = probs_test_exp[i].sort(descending=True)\n",
    "                    # Get last sorted index to be below Q_hat\n",
    "                    pi_stop_i = (sort_i.cumsum(dim=0) <= qhat).sum()\n",
    "                    # Prediction sets\n",
    "                    prediction_set_i = (pi_i[:(pi_stop_i)]).numpy()  # not allow empty sets        \n",
    "\n",
    "                    # - Get expert prediction depending on method\n",
    "                    # ======\n",
    "                    exp_prediction = get_expert_prediction(experts_test, prediction_set_i, method=method)\n",
    "                    # ======\n",
    "                    \n",
    "                    set_size.append(len(prediction_set_i))\n",
    "\n",
    "\n",
    "\n",
    "                # Deferral accuracy: No matter expert ===\n",
    "                exp += (exp_prediction == labels[i])\n",
    "                exp_total += 1\n",
    "                # Individual Expert Accuracy ===\n",
    "                # expert_correct_dic[deferred_exp] += (exp_prediction == labels[i].item())\n",
    "                # expert_total_dic[deferred_exp] += 1\n",
    "                #\n",
    "                correct_sys += (exp_prediction == labels[i])\n",
    "\n",
    "            real_total += 1\n",
    "            \n",
    "\n",
    "        #  ===  Coverage  === #    \n",
    "        cov = 100 * total / real_total\n",
    "\n",
    "        #  === Individual Expert Accuracies === #\n",
    "        expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "                     in range(len(experts_test))}\n",
    "\n",
    "        # Add expert accuracies dict\n",
    "        to_print = {\"coverage\": cov,\n",
    "                    \"system_accuracy\": 100 * correct_sys / real_total,\n",
    "                    \"expert_accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                    \"classifier_accuracy\": 100 * correct / (total + 0.0001),\n",
    "                    \"alone_classifier\": 100 * alone_correct / real_total,\n",
    "                    \"set_size\": set_size,\n",
    "                    \"avg_set_size\": np.mean(set_size),\n",
    "                    \"q_hat\": qhat}\n",
    "        print(to_print, flush=True)\n",
    "\n",
    "        # Save to method dict === \n",
    "        method_dict_ova[method].append(to_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b259b-e787-41e0-b696-9fe694a76eee",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d753c9d-3404-4984-b5c4-9a31b2e8c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Softmax ===\n",
    "n_classes = 10\n",
    "probs_softmax = []\n",
    "confs = []\n",
    "exps = []\n",
    "true = []\n",
    "\n",
    "path = \"softmax_increase_oracle_v2/\"\n",
    "n_experts = 10\n",
    "seed = 436\n",
    "k_list = np.arange(0,10)\n",
    "for k in k_list:\n",
    "    model_name = '_k_' + str(k)\n",
    "    seed_name = 'seed_' + str(seed)\n",
    "    with open(path + 'confidence_multiple_experts' + model_name + seed_name + '.txt', 'r') as f:\n",
    "        conf = json.loads(json.load(f))\n",
    "    with open(path + 'expert_predictions_multiple_experts' + model_name + seed_name + '.txt', 'r') as f:\n",
    "        exp_pred = json.loads(json.load(f))\n",
    "    with open(path + 'true_label_multiple_experts' + model_name + seed_name + '.txt', 'r') as f:\n",
    "        true_label = json.loads(json.load(f))\n",
    "    true.append(true_label['test'])\n",
    "    exps.append(exp_pred['test'])\n",
    "    c = torch.tensor(conf['test'])\n",
    "    print(c.shape)\n",
    "    # DANI Correction ===\n",
    "    c = c.softmax(dim=1)\n",
    "    probs_softmax.append(c)\n",
    "    # DANI Correction ===\n",
    "\n",
    "    temp = 0\n",
    "    for i in range(n_experts):\n",
    "        temp += c[:, (n_classes + n_experts) - (i + 1)]\n",
    "    prob = c / (1.0 - temp).unsqueeze(-1)\n",
    "    confs.append(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccecb5-175f-4489-9d74-8b8a78ab8ad8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Method dict ===\n",
    "method_list_softmax = [\"standard\", \"last\", \"random\", \"voting\"]\n",
    "method_dict_softmax = {\"standard\": [],\n",
    "               \"last\": [],\n",
    "               \"random\": [],\n",
    "               \"voting\": []}\n",
    "\n",
    "for method in method_list_softmax:\n",
    "                        \n",
    "    set_size = []\n",
    "    print(\"Method: {}\\n\".format(method))\n",
    "    for i, k in enumerate(k_list):\n",
    "        # =============\n",
    "        # = Get Probs =\n",
    "        # =============\n",
    "\n",
    "        probs = probs_softmax[i]\n",
    "        experts = exps[i]\n",
    "        experts = experts[::-1]  # reverse order!\n",
    "        y_true = true[-i]\n",
    "\n",
    "        # Val/Calibration ===\n",
    "        probs_val = probs[:n_val, n_classes:]\n",
    "        experts_val = [exp[:n_val] for exp in experts]\n",
    "        y_true_val = y_true[:n_val]\n",
    "\n",
    "        # Test ===\n",
    "        probs_test = probs[n_val:, n_classes:]\n",
    "        experts_test = [exp[n_val:] for exp in experts]\n",
    "        y_true_test = y_true[n_val:]\n",
    "\n",
    "\n",
    "        # =============\n",
    "        # = Conformal =\n",
    "        # =============\n",
    "\n",
    "        # Calculate Q_hat ===\n",
    "\n",
    "        # === Only on deferred samples\n",
    "        _, predicted = torch.max(probs[:n_val].data, 1)\n",
    "        r = (predicted >= n_classes_exp - n_experts)\n",
    "\n",
    "        # Filter \n",
    "        probs_experts = probs_val[r]\n",
    "        experts_val = [np.array(exp)[r] for exp in experts_val]\n",
    "        y_true_val = np.array(y_true_val)[r]\n",
    "\n",
    "        # Model expert probs ===\n",
    "        # Sort J model outputs for experts\n",
    "        sort, pi = probs_experts.sort(dim=1, descending=True)\n",
    "\n",
    "        # Correctness experts ===\n",
    "        # Check if experts are correct \n",
    "        correct_exp = (np.array(experts_val) == np.array(y_true_val)).T\n",
    "        # idx for correct experts: [[0,1,2], [1,2], [], ...]\n",
    "        correct_exp_idx = [np.where(correct_exp_i)[0] for correct_exp_i in correct_exp]\n",
    "\n",
    "        # obtain the last expert to be retrieved. If empty, then add all values.\n",
    "        # indexes are not the real expert index, but the sorted indexes, e.g. [[1, 0 ,2],  [1,0], [], ...]\n",
    "        pi_corr_exp = [probs_experts[i, corr_exp].sort(descending=True)[1] for i, corr_exp in enumerate(correct_exp)]\n",
    "        pi_corr_exp_stop = [pi_corr_exp_i[-1] if len(pi_corr_exp_i)!=0 else -1 for pi_corr_exp_i in pi_corr_exp]  # last expert\n",
    "\n",
    "        # obtain real expert index back, e.g. [2,1,-1,...]\n",
    "        pi_stop = [correct_exp_idx[i][pi_corr_exp_stop_i] if len(correct_exp_idx[i])!=0 else -1 for i, pi_corr_exp_stop_i in enumerate(pi_corr_exp_stop)]\n",
    "\n",
    "        scores = sort.cumsum(dim=1).gather(1, pi.argsort(1))[range(len(torch.tensor(pi_stop))), torch.tensor(pi_stop)]\n",
    "        n_quantile = r.sum()\n",
    "        qhat = torch.quantile(scores, np.ceil((n_quantile + 1) * (1 - alpha)) / n_quantile, interpolation=\"higher\")\n",
    "\n",
    "        print(\"Q_hat {}: {}\".format(k, qhat))\n",
    "\n",
    "\n",
    "        # =============\n",
    "        # = Metrics =\n",
    "        # =============\n",
    "\n",
    "        # === Initalize ====\n",
    "\n",
    "        correct = 0\n",
    "        correct_sys = 0\n",
    "        exp = 0\n",
    "        exp_total = 0\n",
    "        total = 0\n",
    "        real_total = 0\n",
    "        alone_correct = 0\n",
    "\n",
    "        # Individual Expert Accuracies === #\n",
    "        expert_correct_dic = {k: 0 for k in range(len(experts_test))}\n",
    "        expert_total_dic = {k: 0 for k in range(len(experts_test))}\n",
    "\n",
    "        probs_test_exp = probs_test\n",
    "        probs_test_model = probs[n_val:]\n",
    "\n",
    "        # Predicted value \n",
    "        _, predicted = torch.max(probs_test_model.data, 1)\n",
    "\n",
    "        # Classifier alone prediction\n",
    "        _, prediction = torch.max(probs_test_model.data[:, :(n_classes_exp - n_experts)],1)\n",
    "\n",
    "        labels = y_true_test\n",
    "        for i in range(0, n_test):\n",
    "            r = (predicted[i].item() >= n_classes_exp - n_experts)\n",
    "            alone_correct += (prediction[i] == labels[i]).item()\n",
    "\n",
    "            # Non-deferred \n",
    "            if r == 0:\n",
    "                total += 1\n",
    "                correct += (predicted[i] == labels[i]).item()\n",
    "                correct_sys += (predicted[i] == labels[i]).item()\n",
    "\n",
    "            # Deferred \n",
    "            if r == 1:\n",
    "                # Non Conformal prediction ===\n",
    "                if method==\"standard\":\n",
    "                    deferred_exp = (predicted[i] - n_classes).item()  # reverse order, as in loss function\n",
    "                    exp_prediction = experts_test[deferred_exp][i]\n",
    "                    \n",
    "                    # Conformal prediction ===        \n",
    "                else:                \n",
    "                    # Sort J model outputs for experts. sorted probs and sorted indexes\n",
    "                    sort_i, pi_i = probs_test_exp[i].sort(descending=True)\n",
    "                    # Get last sorted index to be below Q_hat\n",
    "                    pi_stop_i = (sort_i.cumsum(dim=0) <= qhat).sum()\n",
    "                    # Prediction sets\n",
    "                    prediction_set_i = (pi_i[:(pi_stop_i)]).numpy()  # not allow empty sets        \n",
    "\n",
    "                    # - Get expert prediction depending on method\n",
    "                    # ======\n",
    "                    exp_prediction = get_expert_prediction(experts_test, prediction_set_i, method=method)\n",
    "                    # ======\n",
    "                    \n",
    "                    set_size.append(len(prediction_set_i))\n",
    "\n",
    "\n",
    "\n",
    "                # Deferral accuracy: No matter expert ===\n",
    "                exp += (exp_prediction == labels[i])\n",
    "                exp_total += 1\n",
    "                # Individual Expert Accuracy ===\n",
    "                # expert_correct_dic[deferred_exp] += (exp_prediction == labels[i].item())\n",
    "                # expert_total_dic[deferred_exp] += 1\n",
    "                #\n",
    "                correct_sys += (exp_prediction == labels[i])\n",
    "\n",
    "            real_total += 1\n",
    "\n",
    "        #  ===  Coverage  === #    \n",
    "        cov = 100 * total / real_total\n",
    "\n",
    "        #  === Individual Expert Accuracies === #\n",
    "        expert_accuracies = {\"expert_{}\".format(str(k)): 100 * expert_correct_dic[k] / (expert_total_dic[k] + 0.0002) for k\n",
    "                     in range(len(experts_test))}\n",
    "\n",
    "        # Add expert accuracies dict\n",
    "        to_print = {\"coverage\": cov,\n",
    "                    \"system_accuracy\": 100 * correct_sys / real_total,\n",
    "                    \"expert_accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                    \"classifier_accuracy\": 100 * correct / (total + 0.0001),\n",
    "                    \"alone_classifier\": 100 * alone_correct / real_total,\n",
    "                    \"set_size\": set_size,\n",
    "                    \"avg_set_size\": np.mean(set_size),\n",
    "                    \"q_hat\": qhat}\n",
    "        print(to_print, flush=True)\n",
    "\n",
    "        # Save to method dict === \n",
    "        method_dict_softmax[method].append(to_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54ac20-5d27-4d36-8637-57b3c5f9971a",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d820b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "# # === Latex Options === #\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "\n",
    "# === Matplotlib Options === #\n",
    "cm = plt.cm.get_cmap('tab10')\n",
    "plot_args = {\"marker\": \"o\",\n",
    "             \"markeredgecolor\": \"k\",\n",
    "             \"markersize\": 10,\n",
    "             \"linewidth\": 8\n",
    "             }\n",
    "sns.set_context(\"talk\", font_scale=1.3)\n",
    "fig_size = (7,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570457ab-0fab-471a-9c47-daf9b8674d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96a176",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_list = np.arange(1,11)\n",
    "\n",
    "\n",
    "# OvA ===\n",
    "sys_acc_standard_ova = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_ova[\"standard\"]])\n",
    "sys_acc_last_ova = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_ova[\"last\"]])\n",
    "sys_acc_random_ova = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_ova[\"random\"]])\n",
    "sys_acc_voting_ova = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_ova[\"voting\"]])\n",
    "\n",
    "# Softmax ===\n",
    "sys_acc_standard_softmax = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_softmax[\"standard\"]])\n",
    "sys_acc_last_softmax = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_softmax[\"last\"]])\n",
    "sys_acc_random_softmax = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_softmax[\"random\"]])\n",
    "sys_acc_voting_softmax = np.array([method_d[\"system_accuracy\"] for method_d in method_dict_softmax[\"voting\"]])\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "# OvA ===\n",
    "ax.plot(k_list, sys_acc_last_ova, \"-\", label=r\"Last\", color=cm(0), **plot_args)\n",
    "ax.plot(k_list, sys_acc_random_ova, \"-\", label=r\"Random\", color=cm(1), **plot_args)\n",
    "ax.plot(k_list, sys_acc_voting_ova, \"-\", label=r\"Voting\", color=cm(2), **plot_args)\n",
    "ax.plot(k_list, sys_acc_standard_ova, \"-\", label=r\"w/o conformal\", color=cm(3), **plot_args)\n",
    "\n",
    "# Softmax ===\n",
    "ax.plot(k_list, sys_acc_last_softmax, \"--\", color=cm(0), **plot_args)\n",
    "ax.plot(k_list, sys_acc_random_softmax, \"--\", color=cm(1), **plot_args)\n",
    "ax.plot(k_list, sys_acc_voting_softmax, \"--\", color=cm(2), **plot_args)\n",
    "ax.plot(k_list, sys_acc_standard_softmax, \"--\", color=cm(3), **plot_args)\n",
    "\n",
    "plt.xticks(k_list, k_list)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'System Acc. ($\\%$)')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"system_acc_increase_oracles.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d8e45",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_list\n",
    "\n",
    "# OvA ===\n",
    "exp_acc_standard_ova = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_ova[\"standard\"]])\n",
    "exp_acc_last_ova = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_ova[\"last\"]])\n",
    "exp_acc_random_ova = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_ova[\"random\"]])\n",
    "exp_acc_voting_ova = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_ova[\"voting\"]])\n",
    "\n",
    "# Softmax ===\n",
    "exp_acc_standard_softmax = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_softmax[\"standard\"]])\n",
    "exp_acc_last_softmax = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_softmax[\"last\"]])\n",
    "exp_acc_random_softmax = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_softmax[\"random\"]])\n",
    "exp_acc_voting_softmax = np.array([method_d[\"expert_accuracy\"] for method_d in method_dict_softmax[\"voting\"]])\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "# OvA ===\n",
    "ax.plot(k_list, exp_acc_last_ova, \"-\", label=r\"Last\", color=cm(0), **plot_args)\n",
    "ax.plot(k_list, exp_acc_random_ova, \"-\", label=r\"Random\", color=cm(1), **plot_args)\n",
    "ax.plot(k_list, exp_acc_voting_ova, \"-\", label=r\"Voting\", color=cm(2), **plot_args)\n",
    "ax.plot(k_list, exp_acc_standard_ova, \"-\", label=r\"w/o conformal\", color=cm(3), **plot_args)\n",
    "\n",
    "# Softmax ===\n",
    "ax.plot(k_list, exp_acc_last_softmax, \"--\", color=cm(0), **plot_args)\n",
    "ax.plot(k_list, exp_acc_random_softmax, \"--\", color=cm(1), **plot_args)\n",
    "ax.plot(k_list, exp_acc_voting_softmax, \"--\", color=cm(2), **plot_args)\n",
    "ax.plot(k_list, exp_acc_standard_softmax, \"--\", color=cm(3), **plot_args)\n",
    "\n",
    "plt.xticks(k_list, k_list)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'Expert Acc. ($\\%$)')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"expert_acc_increase_oracles.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da102664",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_list\n",
    "coverage_ova = np.array([method_d[\"coverage\"] for method_d in method_dict_ova[\"last\"]])\n",
    "coverage_softmax = np.array([method_d[\"coverage\"] for method_d in method_dict_softmax[\"last\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(k_list, coverage_ova, \"-\", label=r\"OvA\", **plot_args)\n",
    "ax.plot(k_list, coverage_softmax, \"--\", label=r\"Softmax\", **plot_args)\n",
    "\n",
    "plt.xticks(k_list, k_list)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'Model Coverage. ($\\%$)')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(\"coverage_increase_oracles.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b7887-fe64-48e4-8b0b-bfaeabbaa033",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.mean(method_d[\"set_size\"]) for method_d in method_dict_ova[\"voting\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffce433-ea4e-49ca-ab22-0461f5fbd946",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list\n",
    "avg_set_size_ova = np.array([method_d[\"avg_set_size\"] for method_d in method_dict_ova[\"last\"]])\n",
    "avg_set_size_softmax = np.array([method_d[\"avg_set_size\"] for method_d in method_dict_softmax[\"last\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(k_list, avg_set_size_ova, \"-\", label=r\"OvA\", **plot_args)\n",
    "ax.plot(k_list, avg_set_size_softmax, \"--\", label=r\"Softmax\", **plot_args)\n",
    "\n",
    "plt.xticks(k_list, k_list)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'Avg. Set Size')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(\"avg_set_size_increase_oracles.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df59ad-e9f5-43d5-978c-1e8c9efd323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([method_d[\"q_hat\"] for method_d in method_dict_ova[\"last\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dade5ae-c3a1-46d1-8591-3f78b6f3fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list\n",
    "q_ova = np.array([method_d[\"q_hat\"] for method_d in method_dict_ova[\"last\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(k_list, q_ova, \"-\", label=r\"OvA\", **plot_args)\n",
    "\n",
    "plt.xticks(k_list, k_list)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'Qhat')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(\"qhat_ova_increase_oracles.pdf\")\n",
    "\n",
    "\n",
    "q_ova_softmax = np.array([method_d[\"q_hat\"] for method_d in method_dict_softmax[\"last\"]])\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "ax.plot(k_list, q_ova_softmax, \"--\", label=r\"Softmax\", **plot_args)\n",
    "\n",
    "plt.xticks(k_list, k_list)\n",
    "plt.yticks(list(plt.yticks()[0])[::2])\n",
    "plt.ylabel(r'Qhat')\n",
    "plt.xlabel(r'Prob Experts')\n",
    "plt.title(r\"CIFAR-10\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "f.set_tight_layout(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(\"qhat_softmax_increase_oracles.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc7c0d-70c5-4ba5-8042-998b73d54c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
